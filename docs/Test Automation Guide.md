# Test Automation Guide

This guide outlines the testing strategies and procedures implemented within this codebase to ensure the reliability, correctness, and resilience of its components and workflows. It covers automated tests for utility functions and comprehensive end-to-end testing for critical, distributed systems like the Product Quality Alert System.

## 1. Testing Philosophy

The testing philosophy embraces a combination of unit-level validation for individual components and robust integration/end-to-end testing for complex, distributed systems. Emphasis is placed on:

*   **Early Detection of Issues**: Automated tests are run frequently to catch regressions and bugs as soon as they are introduced.
*   **Comprehensive Coverage**: Tests address normal, edge, and various error cases to ensure robust behavior.
*   **Observable System Behavior**: Detailed logging is a primary mechanism for debugging, validating system flow, and understanding error conditions.
*   **Resilience Testing**: Verifying automatic retry mechanisms and graceful error handling in distributed systems is crucial.

## 2. Test Environments

Tests are designed to be runnable in local development environments and target deployed Google Cloud resources for integration and end-to-end scenarios. For the Product Quality Alert System, tests specifically interact with the deployed Cloud Function, Pub/Sub topics, and the external JIRA service.

## 3. How to Run Tests

### 3.1. Utility Function Testing: `add_two_numbers`

This section details how to test the `add_two_numbers` function, which is designed to safely sum two numerical inputs while providing clear logging of its operations. The `demo_log_examples.py` script provides a comprehensive suite of scenarios.

*   **Location**: `demo_log_examples.py`
*   **Purpose**: To verify the `add_two_numbers` function's behavior across various valid, invalid, and edge-case inputs, and to confirm correct logging output (INFO/ERROR messages and correlation IDs).

**Execution:**

Navigate to the directory containing `demo_log_examples.py` and execute the script:

```bash
python demo_log_examples.py
```

**Expected Output:**

The script will print a series of detailed output blocks, each corresponding to a specific test scenario. For each scenario, it will display:
*   The scenario description.
*   The `add_two_numbers` function call with its inputs.
*   The log messages generated by the function (including `INFO` and `ERROR` levels, and correlation IDs where applicable).
*   The return value of the function.

This includes tests for:
*   Normal successful additions.
*   `ValueError` cases (e.g., non-numeric strings like `"hello"`, empty strings, whitespace, float-like strings such as `"3.14"` which will be truncated after `float` conversion, and `"1e5"` for scientific notation).
*   `None` inputs.
*   Unicode digits (e.g., `"ï¼•"`).
*   Hexadecimal strings (e.g., `"0xFF"`).
*   Very large numbers.
*   Custom and empty correlation IDs to validate logging prefixes.
*   Potential log injection attempts, demonstrating robust handling.

**Key Takeaways from `demo_log_examples.py`:**
The `add_two_numbers` function, as implemented in `addNums.py`, gracefully handles various input types by attempting conversion to `float` then `int`, and logs errors for non-convertible inputs, ensuring the system's observable behavior is predictable.

### 3.2. Product Quality Alert System: End-to-End Integration Tests

This critical workflow involves multiple distributed components: BigQuery Analytics â†’ Pub/Sub â†’ Cloud Function â†’ JIRA. Testing this system requires simulating an alert and observing the resulting JIRA ticket creation and associated logs.

#### 3.2.1. Checking JIRA Service Status and Credentials

Before triggering alerts, it is crucial to confirm that the JIRA service is accessible and the Cloud Function's credentials are valid.

*   **Location**: `cloud_function_jira_processor/test_jira_status.py`
*   **Purpose**: To verify basic connectivity to the JIRA server, accessibility of its API endpoints, and successful authentication using credentials retrieved from Google Secret Manager.

**Execution:**

Navigate to the `cloud_function_jira_processor` directory and run the test script:

```bash
cd cloud_function_jira_processor
python test_jira_status.py
```

**Expected Output (if JIRA is online and credentials are valid):**

```
============================================================
JIRA Service Status Check
============================================================

1ï¸âƒ£  Testing basic connectivity...
   âœ… JIRA site is reachable (HTTP 200)

2ï¸âƒ£  Testing API endpoint...
   âœ… JIRA API is accessible (HTTP 200)
   ðŸ“Š Server: Jira
   ðŸ”¢ Version: X.Y.Z (version will vary based on JIRA instance)

3ï¸âƒ£  Testing JIRA credentials...
   âœ… Retrieved API key from Secret Manager
   ðŸ“ Key length: 192 characters
   âœ… Successfully authenticated as: joseph.shorter@sada.com
   ðŸ“§ Email: joseph.shorter@sada.com

============================================================
âœ… JIRA is ONLINE and credentials are VALID
============================================================

ðŸŽ¯ Next Steps:
   1. Cloud Function will automatically process queued messages
   2. Or publish new alerts to trigger ticket creation
   3. Check Cloud Function logs for execution details
```

If JIRA is currently unavailable or experiencing issues, the script will report `âŒ JIRA SERVICE IS CURRENTLY UNAVAILABLE` and provide details on the automatic retry mechanism.

#### 3.2.2. Publishing Test Alerts

To simulate an end-to-end flow and trigger the Cloud Function, you can publish a test alert to the Pub/Sub topic.

*   **Location**: The `detect_product_quality_issues` and `publish_product_alert` functions are part of the `agent` module (e.g., `m_analytical_demo/agent.py`).
*   **Purpose**: To trigger the `product-quality-jira-processor` Cloud Function by publishing a simulated product quality alert message to the `product-quality-alerts` Pub/Sub topic.

**Prerequisites:**
*   Ensure you are in the `/Users/joseph.shorter/repos/m_analytical_demo` directory (or adjust the path as necessary).
*   Activate the appropriate Python virtual environment that contains the `agent` module and its dependencies.

**Execution:**

```bash
cd /Users/joseph.shorter/repos/m_analytical_demo
source /Users/joseph.shorter/repos/.venv/bin/activate # Adjust path to your virtual environment

# Option 1: Basic execution
python -c "from agent import detect_product_quality_issues, publish_product_alert; \
           alerts = detect_product_quality_issues(7, 'high'); \
           publish_product_alert(alerts)"

# Option 2: More verbose output during testing
python -c "from agent import detect_product_quality_issues, publish_product_alert; \
           import json; \
           issues = detect_product_quality_issues(7, 'high'); \
           data = json.loads(issues); \
           print(f'Detected {data[\"issues_found\"]} issues'); \
           result = publish_product_alert(issues); \
           print(result)"
```

This command will:
1.  Simulate the detection of product quality issues for a 7-day period with "high" severity.
2.  Publish these issues as an alert message to the `product-quality-alerts` Pub/Sub topic.

#### 3.2.3. Monitoring Cloud Function Logs

After publishing an alert message, the Cloud Function logs are the primary place to observe its execution and confirm the attempt (or success) of JIRA ticket creation.

*   **Purpose**: To verify that the `product-quality-jira-processor` Cloud Function receives the Pub/Sub message, processes it, attempts to create a JIRA ticket, and logs the outcome (success, failure, or retry initiation).

**Execution:**

Open a separate terminal and continuously monitor the Cloud Function logs using `gcloud`:

```bash
gcloud functions logs read product-quality-jira-processor \
  --region=us-central1 \
  --gen2 \
  --limit=50 \
  --format="value(severity,log)"
```

**Expected Output:**

Upon successful processing and JIRA ticket creation, you should observe logs similar to:

```
INFO: ðŸ“¨ Received alert from Pub/Sub
INFO: ðŸ” Processing: Fisher-Price Code-a-Pillar (ID: ..., Severity: HIGH, Revenue at Risk: $X,XXX.XX)
INFO: âœ… JIRA ticket created: AITD-XXX for Fisher-Price Code-a-Pillar (severity: HIGH)
INFO:    URL: https://sadaadvservices.atlassian.net/browse/AITD-XXX
INFO:    Product ID: ..., Revenue at Risk: $X,XXX.XX
```

If JIRA is unavailable or an error occurs during ticket creation, you will see `ERROR` and `WARNING` messages indicating the failure and potentially the Pub/Sub retry mechanism being triggered.

#### 3.2.4. Verifying JIRA Tickets

The final step in the end-to-end test is to check JIRA directly to confirm that tickets have been created as expected.

*   **Purpose**: To visually confirm the creation of new JIRA tickets with the correct summary, description (populated with alert details), issue type, priority, and labels based on the alert payload.

**Action:**

Access your JIRA project in a web browser:
*   `https://sadaadvservices.atlassian.net/browse/AITD`

Look for new tickets with summaries following the pattern: "Product Quality Alert: \[Product Name] - \[Severity] Severity".

#### 3.2.5. Monitoring Pub/Sub Subscription

To understand the state of messages being processed or awaiting retry, monitoring the Pub/Sub subscription is essential, especially during periods of external service unavailability (e.g., JIRA downtime).

*   **Purpose**: To check for undelivered messages, message backlog, and overall subscription health.

**Execution:**

```bash
# Check messages waiting in queue
gcloud pubsub subscriptions describe \
  eventarc-us-central1-product-quality-jira-processor-612217-sub-916 \
  --project=sada-joseph-shorter-sada

# View subscription metrics (e.g., undelivered messages over time)
gcloud monitoring time-series list \
  --filter='metric.type="pubsub.googleapis.com/subscription/num_undelivered_messages"' \
  --project=sada-joseph-shorter-sada
```

**Expected Output:**

The `describe` command will provide details such as `num_undelivered_messages`, `ack_deadline`, and `message_retention_duration`. The `time-series list` command will provide historical metrics on message backlog, allowing you to observe if messages are accumulating or being processed effectively.

### 3.3. Resilience Testing

The Product Quality Alert System is designed with built-in resilience mechanisms. Testing these features involves simulating failures and observing the system's automatic recovery.

**Key features to test:**

*   **Exponential Backoff**: Pub/Sub automatically retries messages with increasing delays following a failure (e.g., JIRA unavailability), up to a maximum backoff of 600 seconds (10 minutes).
*   **Message Retention**: Pub/Sub retains messages for up to 7 days, ensuring that transient outages of external services do not lead to data loss.
*   **Smart Error Detection**: The Cloud Function (`cloud_function_jira_processor/main.py`) differentiates between transient (retryable) and permanent (non-retryable) errors. Transient errors (e.g., JIRA `404`, `502`, `503` responses, `timeout`, `unavailable` connection errors) trigger Pub/Sub retries. Permanent errors (e.g., malformed JSON payload that cannot be parsed) are logged but do not trigger retries to prevent infinite loops.

**How to Test Resilience:**
1.  **Simulate JIRA Unavailability**: Temporarily introduce a configuration error in the Cloud Function's environment variables (e.g., an invalid `JIRA_SERVER` URL or `USER_EMAIL`) or block network access to JIRA.
2.  **Publish Alerts**: Publish several test alerts as described in Section 3.2.2.
3.  **Monitor Logs and Pub/Sub**: Observe the Cloud Function logs (`3.2.3`) for error messages indicating JIRA unavailability and `WARNING` messages confirming that the message will be retried. Monitor Pub/Sub subscription metrics (`3.2.5`) to see `num_undelivered_messages` increasing and then decreasing as retries occur.
4.  **Restore JIRA**: Correct the configuration error or restore network access to JIRA.
5.  **Observe Recovery**: Within the Pub/Sub retry window, the Cloud Function should automatically process the queued messages and create the corresponding JIRA tickets without manual intervention.

## 4. Troubleshooting Testing Issues

### Cloud Function Not Triggering

*   **Check Pub/Sub Topic Existence**:
    ```bash
    gcloud pubsub topics describe product-quality-alerts
    ```
*   **Check Cloud Function Status**:
    ```bash
    gcloud functions describe product-quality-jira-processor --region=us-central1 --gen2
    ```

### JIRA Tickets Not Created

*   **Review Cloud Function Logs for Specific Errors**: Filter logs for warning and error messages:
    ```bash
    gcloud functions logs read product-quality-jira-processor \
        --region=us-central1 \
        --gen2 \
        --limit=20 \
        --filter="severity>=WARNING"
    ```
*   **Test Secret Manager Access**: Ensure the Cloud Function's service account can access the JIRA API key:
    ```bash
    gcloud secrets versions access latest --secret=JIRA_API_KEY
    ```
*   **Verify Environment Variables**: Confirm that the Cloud Function's environment variables (e.g., `JIRA_PROJECT_ID`, `JIRA_SERVER`) are correctly configured:
    ```bash
    gcloud functions describe product-quality-jira-processor \
        --region=us-central1 \
        --gen2 \
        --format="value(serviceConfig.environmentVariables)"
    ```

### High Error Rate in Cloud Function

*   Use the Cloud Function logs command (mentioned above) to identify common error patterns or specific exceptions.
*   Check Pub/Sub subscription metrics (`3.2.5`) for `num_undelivered_messages` or `expired_messages` to see if messages are backing up or failing permanently (indicating a non-retryable error or a prolonged outage).

## 5. Future Test Automation Enhancements

To further enhance the robustness and maintainability of the system, consider the following test automation improvements:

1.  **Automated End-to-End Test Suite**: Develop a dedicated, comprehensive Python script (similar to `test_jira_status.py`) that:
    *   Publishes a carefully crafted test alert message.
    *   Monitors Cloud Function logs programmatically for successful execution or expected failure/retry.
    *   Optionally queries the JIRA API to confirm ticket creation, or verifies Pub/Sub metrics for messages in retry.
    *   Cleans up test data (e.g., deleting created JIRA tickets).
2.  **Mocking/Stubbing for Unit Tests**: For `cloud_function_jira_processor/main.py`, introduce proper unit tests that mock external dependencies (e.g., JIRA API client, Secret Manager client) to test `create_product_quality_ticket` and `process_quality_alert` logic in isolation without actual external calls.
3.  **Performance Testing**: Implement tools to simulate high volumes of alerts and assess the system's performance, scalability, and identify potential bottlenecks under load.
4.  **Monitoring Alerts for Test Failures**: Configure Cloud Monitoring alerts for critical metrics such as Cloud Function error rates and Pub/Sub message backlog, providing proactive notifications if tests reveal system degradation.
5.  **Integration with CI/CD**: Integrate all automated test scripts into a CI/CD pipeline to automatically run tests on every code change, ensuring continuous quality and rapid feedback.